"""
Performs clustering (K-Means, HDBSCAN) on text representations.
Loads data from the 'output' directory generated by text_representation.py.
Includes Elbow method plot generation, 3D PCA, and 2D UMAP visualizations.
"""

import glob  # Add glob for file searching
import os

import hdbscan  # type: ignore
import joblib  # type: ignore
import matplotlib.pyplot as plt  # type: ignore
import numpy as np  # type: ignore
import pandas as pd  # type: ignore
import scipy.sparse  # type: ignore
import umap.umap_ as umap  # type: ignore
from hdbscan.validity import validity_index  # type: ignore
from sklearn.cluster import KMeans  # type: ignore
from sklearn.decomposition import PCA  # type: ignore
from sklearn.metrics import silhouette_score  # type: ignore

# --- Configuration ---
OUTPUT_DIR = "src/clustering_output"  # Directory where CLUSTERING results will be stored
VECTORIZER_OUTPUT_DIR = "src/vectorizer_output"  # Directory where REPRESENTATIONS are saved

# Create Output Directory for Clustering Results
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)
    print(f"Created clustering output directory: {OUTPUT_DIR}")

# Clustering Parameters (start with reasonable defaults, tune as needed)
# Elbow plots are generated to help choose this value - modify it based on the plots.
KMEANS_N_CLUSTERS = 5
ELBOW_MAX_K = 15  # Maximum K to check for the elbow plot
HDBSCAN_MIN_CLUSTER_SIZE = 2  # Example: Minimum size for HDBSCAN clusters

# Visualization settings
VISUALIZE = True  # Set to False to skip generating plots
PCA_N_COMPONENTS = 3  # Reduce to 3D for plotting
UMAP_N_COMPONENTS = 2  # Reduce to 2D for UMAP plotting


# --- Helper Functions ---
def load_representations(vectorizer_output_dir):
    """Loads TF-IDF, filenames, and all embedding representations.

    Args:
        vectorizer_output_dir (str): Directory where representations are saved.

    Returns:
        tuple: (tfidf_matrix, filenames, embeddings_dict)
               tfidf_matrix: Sparse matrix or None (if file doesn't exist)
               filenames: List of strings
               embeddings_dict: Dict {model_key: embedding_matrix}
    """
    print(f"Loading representations from: {vectorizer_output_dir}")
    tfidf_matrix_path = os.path.join(vectorizer_output_dir, "tfidf_matrix.npz")
    filenames_path = os.path.join(vectorizer_output_dir, "filenames.joblib")
    embeddings_pattern = os.path.join(vectorizer_output_dir, "embeddings_*.npy")

    loaded_tfidf_matrix = None
    embeddings_dict = {}

    # Load Filenames (Required)
    print(f"  Attempting to load filenames from {filenames_path}")
    loaded_filenames = joblib.load(filenames_path)
    print(f"  Loaded filenames ({len(loaded_filenames)} documents)")

    # Load TF-IDF (Optional - check existence first)
    print(f"Checking for TF-IDF matrix at {tfidf_matrix_path}")
    if os.path.exists(tfidf_matrix_path):
        loaded_tfidf_matrix = scipy.sparse.load_npz(tfidf_matrix_path)
        print(f"  Loaded TF-IDF matrix ({loaded_tfidf_matrix.shape})")
        # Check TF-IDF shape against filenames
        if loaded_tfidf_matrix.shape[0] != len(loaded_filenames):
            raise ValueError(
                f"TF-IDF matrix shape ({loaded_tfidf_matrix.shape[0]}) does not match "
                f"filenames count ({len(loaded_filenames)})"
            )

    # Load Embeddings
    embedding_files = glob.glob(embeddings_pattern)
    print(f"  Found {len(embedding_files)} potential embedding files matching pattern.")

    for emb_path in embedding_files:
        filename = os.path.basename(emb_path)
        print(f"    Attempting to load: {filename}")
        if filename.startswith("embeddings_") and filename.endswith(".npy"):
            model_key = filename[len("embeddings_") : -len(".npy")]
            embedding_matrix = np.load(emb_path)
            print(f"      Loaded '{model_key}' embeddings ({embedding_matrix.shape})")

            # Basic check against filenames count
            if embedding_matrix.shape[0] != len(loaded_filenames):
                raise ValueError(
                    f"Embedding matrix '{model_key}' shape ({embedding_matrix.shape[0]}) "
                    f"does not match filenames count ({len(loaded_filenames)})"
                )
            embeddings_dict[model_key] = embedding_matrix
        else:
            print(f"Skipping file (unexpected name format): {emb_path}")

    # Check if at least one representation was loaded (TF-IDF or some embeddings)
    if loaded_tfidf_matrix is None and not embeddings_dict:
        raise FileNotFoundError(
            f"Failed to load TF-IDF and found no valid embedding files in {vectorizer_output_dir}"
        )

    print("Representation loading complete.")
    return loaded_tfidf_matrix, loaded_filenames, embeddings_dict


def plot_elbow_method(data, max_k, title_suffix, filename, random_state=42):
    """Calculates inertia for different k and plots the elbow curve."""
    print(f"\nGenerating Elbow Method plot for: {title_suffix}")
    if (
        scipy.sparse.issparse(data) and data.shape[0] * data.shape[1] > 1e8
    ):  # Heuristic for large sparse data
        print("  Warning: Data is large and sparse, Elbow method might be slow.")

    inertias = []
    k_range = range(2, max_k + 1)
    for k in k_range:
        kmeans = KMeans(n_clusters=k, random_state=random_state, n_init=10)
        kmeans.fit(data)
        inertias.append(kmeans.inertia_)
        print(f"  Calculated inertia for k={k}")

    # Filter out NaN values for plotting if any errors occurred
    valid_indices = ~np.isnan(inertias)
    valid_k = [k_val for i, k_val in enumerate(k_range) if valid_indices[i]]
    valid_inertias = [inert for i, inert in enumerate(inertias) if valid_indices[i]]

    if not valid_inertias:
        print("  No valid inertia values calculated. Skipping Elbow plot.")
        return

    plt.figure(figsize=(8, 5))
    plt.plot(valid_k, valid_inertias, "bo-")
    plt.xlabel("Number of clusters (k)")
    plt.ylabel("Inertia (Within-cluster sum of squares)")
    plt.title(f"Elbow Method for Optimal K ({title_suffix})")
    plt.grid(True)
    plt.xticks(valid_k)

    plt.savefig(filename, dpi=150, bbox_inches="tight")
    print(f"  Elbow plot saved to {filename}")
    plt.close()


def plot_silhouette_scores(k_scores, title_suffix, filename):
    """Plots Silhouette scores for different k values.

    Args:
        k_scores (dict): Dictionary where keys are K values and values are Silhouette scores.
        title_suffix (str): String to append to the plot title.
        filename (str): Path to save the plot image.
    """
    if not k_scores:
        print("  No Silhouette scores provided. Skipping plot.")
        return

    # Filter out K values where score calculation failed (e.g., score is -1 or None)
    valid_k = sorted([k for k, score in k_scores.items() if score is not None and score > -1.0])
    valid_scores = [k_scores[k] for k in valid_k]

    if not valid_k:
        print(f"  No valid Silhouette scores > -1 found for {title_suffix}. Skipping plot.")
        return

    plt.figure(figsize=(8, 5))
    plt.plot(valid_k, valid_scores, "bo-")
    plt.xlabel("Number of clusters (k)")
    plt.ylabel("Silhouette Score")
    plt.title(f"Silhouette Score vs. K ({title_suffix})")
    plt.grid(True)
    # Ensure x-ticks match the K values tested
    plt.xticks(range(min(valid_k), max(valid_k) + 1))

    plt.savefig(filename, dpi=150, bbox_inches="tight")
    print(f"  Silhouette score plot saved to {filename}")
    plt.close()


def run_kmeans(data, n_clusters, random_state=42):
    """Runs KMeans clustering and calculates silhouette score."""
    print(f"\nRunning KMeans with k={n_clusters}...")
    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=10)
    labels = kmeans.fit_predict(data)

    # Placeholder if score calculation fails
    score = -999

    if len(set(labels)) > 1:
        score = silhouette_score(data, labels)
        print(f"Inertia: {kmeans.inertia_:.2f}")
        print(f"Silhouette Score: {score:.4f}")
    else:
        print("Only 1 cluster found, Silhouette score is not defined.")

    unique_labels, counts = np.unique(labels, return_counts=True)
    print(f"  Cluster label counts: {dict(zip(unique_labels, counts))}")
    return labels, score


def run_hdbscan(data, min_cluster_size):
    """Runs HDBSCAN clustering and calculates DBCV score.

    Args:
        data: Data matrix (dense numpy array recommended for DBCV).
        min_cluster_size (int): Minimum cluster size for HDBSCAN.

    Returns:
        tuple: (labels, dbcv_score)
    """
    print(f"\nRunning HDBSCAN with min_cluster_size={min_cluster_size}...")

    # Ensure data is dense numpy array for validity_index
    if scipy.sparse.issparse(data):
        print("  Converting sparse data to dense for HDBSCAN/DBCV...")
        data_dense = data.toarray()
    elif not isinstance(data, np.ndarray):
        print("  Converting data to NumPy array for HDBSCAN/DBCV...")
        data_dense = np.array(data)
    else:
        data_dense = data  # Already dense numpy

    # Ensure correct dtype for HDBSCAN/DBCV internals
    if data_dense.dtype != np.float64:
        print(f"  Converting data dtype from {data_dense.dtype} to float64 for DBCV...")
        data_dense = data_dense.astype(np.float64)

    clusterer = hdbscan.HDBSCAN(
        min_cluster_size=min_cluster_size,
        metric="euclidean",  # Ensure metric matches DBCV expectation if needed
        # gen_min_span_tree=True # Required by some validity metrics, not DBCV
    )

    labels = clusterer.fit_predict(data_dense)
    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
    n_noise = np.sum(labels == -1)

    print(f"  Clusters found: {n_clusters}")
    print(f"  Noise points: {n_noise} ({n_noise / len(labels):.2%})" if len(labels) > 0 else "")

    # Calculate DBCV score
    dbcv_score = np.nan  # Use NaN as default/placeholder
    # DBCV works best with dense data and Euclidean distance
    if n_clusters > 0:  # DBCV requires at least one cluster (besides noise)
        print("  Calculating DBCV Score...")
        dbcv_score = validity_index(data_dense, labels, metric="euclidean")  # Let errors propagate
        print(f"  DBCV Score: {dbcv_score:.4f}")
    else:
        print("  DBCV Score: N/A (No clusters found)")

    return labels, dbcv_score


def visualize_clusters_pca_3d(data, labels, title, filename):
    """Visualizes clusters using PCA (3D) for dimensionality reduction."""
    print(f"\nGenerating 3D PCA visualization for: {title}")
    if data.shape[1] < PCA_N_COMPONENTS:
        print(
            f"  Skipping visualization: Data dimension ({data.shape[1]}) \
                is less than PCA components ({PCA_N_COMPONENTS})."
        )
        return

    pca = PCA(n_components=PCA_N_COMPONENTS)

    # Convert to dense for PCA - check memory implications
    if scipy.sparse.issparse(data):
        print("  Applying PCA to sparse data (converting to dense)...")
        try:
            data_dense = data.toarray()
            data_reduced = pca.fit_transform(data_dense)
            del data_dense  # Free memory
        except MemoryError:
            print(
                "  MemoryError: Cannot convert sparse matrix to dense for PCA. Skipping PCA plot."
            )
            return
    else:
        print("  Applying PCA to dense data...")
        data_reduced = pca.fit_transform(data)

    explained_variance = sum(pca.explained_variance_ratio_)
    print(
        f"  PCA explained variance ratio (Top {PCA_N_COMPONENTS}): {pca.explained_variance_ratio_} "
        f"(Total: {explained_variance:.2%})"
    )

    fig = plt.figure(figsize=(12, 10))
    ax = fig.add_subplot(111, projection="3d")

    unique_labels = sorted(list(set(labels)))
    # Get a colormap
    cmap = plt.get_cmap("viridis", len(unique_labels))

    for i, k in enumerate(unique_labels):
        class_member_mask = labels == k
        xy = data_reduced[class_member_mask]

        label_text = f"Cluster {k} ({np.sum(class_member_mask)})"
        color_val = cmap(i)
        marker_size = 30
        alpha_val = 0.7

        ax.scatter(
            xy[:, 0],
            xy[:, 1],
            xy[:, 2],
            s=marker_size,
            c=[color_val],
            label=label_text,
            alpha=alpha_val,
        )

    ax.set_title(f"{title}\n(3D PCA Reduced - {explained_variance:.1%} Explained Variance)")
    ax.set_xlabel("Principal Component 1")
    ax.set_ylabel("Principal Component 2")
    ax.set_zlabel("Principal Component 3")
    ax.legend(loc="best", fontsize="small")
    ax.view_init(elev=20.0, azim=-35)

    plt.savefig(filename, dpi=150, bbox_inches="tight")
    print(f"  3D PCA Visualization saved to {filename}")
    plt.close(fig)  # Close figure to free memory


def visualize_clusters_umap(data, labels, title, filename):
    """Visualizes clusters using UMAP (2D) for dimensionality reduction."""
    print(f"\nGenerating 2D UMAP visualization for: {title}")

    reducer = umap.UMAP(
        n_components=UMAP_N_COMPONENTS, random_state=42, n_neighbors=15, min_dist=0.1
    )
    print("  Applying UMAP (n_neighbors=15, min_dist=0.1) reduction...")
    data_reduced = reducer.fit_transform(data)
    print(f"  UMAP reduction complete. Reduced shape: {data_reduced.shape}")

    plt.figure(figsize=(10, 8))
    unique_labels = sorted(list(set(labels)))
    cmap = plt.get_cmap("viridis", len(unique_labels))

    for i, k in enumerate(unique_labels):
        class_member_mask = labels == k
        xy = data_reduced[class_member_mask]

        label_text = f"Cluster {k} ({np.sum(class_member_mask)})"
        color_val = cmap(i)
        marker_size = 30
        alpha_val = 0.7

        if k == -1:
            label_text = f"Noise ({np.sum(class_member_mask)})"
            color_val = (0.5, 0.5, 0.5, 0.3)
            marker_size = 10
            alpha_val = 0.3

        plt.scatter(
            xy[:, 0], xy[:, 1], s=marker_size, c=[color_val], label=label_text, alpha=alpha_val
        )

    plt.title(f"{title} (UMAP Reduced)")
    plt.xlabel("UMAP Component 1")
    plt.ylabel("UMAP Component 2")
    plt.legend(loc="best", fontsize="small")
    plt.grid(True)

    plt.savefig(filename, dpi=150, bbox_inches="tight")
    print(f"  UMAP Visualization saved to {filename}")
    plt.close()


def save_results(labels_dict, output_dir):
    """Saves clustering labels and summary (including DBCV) to files."""
    print("\nSaving clustering results...")
    results_data = {}
    summary_columns = [
        "Algorithm",
        "Representation",
        "Num Clusters",
        "Noise Points",
        "Silhouette Score",
        "DBCV Score",
    ]

    for name, data in labels_dict.items():
        labels = data["labels"]
        path = os.path.join(output_dir, f"{name}_labels.joblib")
        joblib.dump(labels, path)
        print(f"  Saved {name} labels to {path}")
        # Prepare data for summary dataframe
        results_data[name] = {
            "Algorithm": data["algorithm"],
            "Representation": data["representation"],
            "Num Clusters": len(set(label for label in labels if label != -1)),
            "Noise Points": np.sum(labels == -1),
            "Silhouette Score": data.get("silhouette", np.nan),  # Use NaN for missing
            "DBCV Score": data.get("dbcv_score", np.nan),  # Use NaN for missing
        }

    # Create and save summary dataframe
    summary_df = pd.DataFrame.from_dict(results_data, orient="index", columns=summary_columns)
    # Format scores for display
    summary_df["Silhouette Score"] = summary_df["Silhouette Score"].map("{:.4f}".format)
    summary_df["DBCV Score"] = summary_df["DBCV Score"].map("{:.4f}".format)

    summary_path = os.path.join(output_dir, "clustering_summary.csv")
    summary_df.to_csv(summary_path)
    print(f"  Saved clustering summary to {summary_path}")


# --- K-Means Specific Helpers ---
def find_optimal_k_silhouette(data, max_k, random_state=42):
    """Finds the optimal K for KMeans based on Silhouette Score.

    Args:
        data: The data matrix (sparse or dense).
        max_k (int): The maximum number of clusters to test.
        random_state (int): Random seed for KMeans.

    Returns:
        tuple: (best_k, best_score, scores_dict)
               best_k yields the highest Silhouette Score.
               scores_dict contains {k: score} mapping.
               Returns (2, -1, {}) if no valid score > -1 is found.
    """
    print(f"  Calculating Silhouette scores for K=2 to {max_k}...")
    scores_dict = {}
    best_k = 2
    best_score = -1.0  # Silhouette score ranges from -1 to 1

    k_range = range(2, max_k + 1)
    for k in k_range:
        # Let potential errors during KMeans or silhouette calculation propagate
        kmeans = KMeans(n_clusters=k, random_state=random_state, n_init=10)
        labels = kmeans.fit_predict(data)

        # Silhouette score requires at least 2 clusters
        if len(set(labels)) < 2:
            print(f"    K={k}: Only 1 cluster found, Silhouette undefined (score=-1)")
            current_score = -1.0
        else:
            current_score = silhouette_score(data, labels)
            print(f"    K={k}: Silhouette Score = {current_score:.4f}")

        scores_dict[k] = current_score  # Store score regardless of validity for plotting
        if current_score > best_score:
            best_score = current_score
            best_k = k

    print(f"  Completed Silhouette analysis. Best K={best_k} with score={best_score:.4f}")
    if best_score <= -1.0:
        print("  Warning: No valid Silhouette score > -1 found. Defaulting K to 2.")
        return 2, -1.0, scores_dict

    return best_k, best_score, scores_dict


# --- Main Execution ---
if __name__ == "__main__":
    print("--- Starting Clustering Script ---")

    # 1. Load Representations
    tfidf_matrix, filenames, embeddings_dict = load_representations(VECTORIZER_OUTPUT_DIR)

    if filenames is None or (tfidf_matrix is None and not embeddings_dict):
        print(
            "\nExiting due to critical loading errors (missing filenames or all representations)."
        )
        exit(1)

    # --- Generate Elbow and Silhouette Plots ---
    print("\n--- Generating Elbow Method & Silhouette Score Plots ---")
    # Plots for TF-IDF (if loaded)
    if tfidf_matrix is not None:
        print("Generating plots for TF-IDF...")
        plot_elbow_method(
            tfidf_matrix,
            ELBOW_MAX_K,
            "TF-IDF",
            os.path.join(OUTPUT_DIR, "elbow_tfidf.png"),
        )
        # Find optimal K and get scores dict for plotting
        _, _, tfidf_silhouette_scores = find_optimal_k_silhouette(tfidf_matrix, ELBOW_MAX_K)
        plot_silhouette_scores(
            tfidf_silhouette_scores, "TF-IDF", os.path.join(OUTPUT_DIR, "silhouette_tfidf.png")
        )
    else:
        print("Skipping plots for TF-IDF (not loaded).")

    # Plots for each Embedding
    for model_key, embeddings in embeddings_dict.items():
        print(f"Generating plots for Embeddings ({model_key})...")
        plot_elbow_method(
            embeddings,
            ELBOW_MAX_K,
            f"Embeddings ({model_key})",
            os.path.join(OUTPUT_DIR, f"elbow_embeddings_{model_key}.png"),
        )
        # Find optimal K and get scores dict for plotting
        _, _, emb_silhouette_scores = find_optimal_k_silhouette(embeddings, ELBOW_MAX_K)
        plot_silhouette_scores(
            emb_silhouette_scores,
            f"Embeddings ({model_key})",
            os.path.join(OUTPUT_DIR, f"silhouette_embeddings_{model_key}.png"),
        )

    # Elbow plots are generated for informational comparison.
    print(
        "\nNote: Elbow plots generated for informational purposes. "
        "The actual K used for KMeans is determined by maximizing the Silhouette Score."
    )

    # --- Perform Clustering ---
    results = {}  # Dictionary to store labels and info

    # 2. Run Clustering on TF-IDF (if loaded)
    if tfidf_matrix is not None:
        print("\n--- Clustering on TF-IDF Representation ---")
        # Determine optimal K for TF-IDF using Silhouette
        print("Finding optimal K for KMeans on TF-IDF...")
        # Unpack all 3 return values, even if scores_dict isn't used here directly
        optimal_k_tfidf, _, _tfidf_scores = find_optimal_k_silhouette(tfidf_matrix, ELBOW_MAX_K)
        print(f"Optimal K for TF-IDF (Silhouette): {optimal_k_tfidf}")

        # Run KMeans with optimal K
        kmeans_tfidf_labels, kmeans_tfidf_sil = run_kmeans(tfidf_matrix, optimal_k_tfidf)
        results["kmeans_tfidf"] = {
            "labels": kmeans_tfidf_labels,
            "silhouette": kmeans_tfidf_sil,  # Note: run_kmeans recalculates this
            "algorithm": "KMeans",
            "representation": "TF-IDF",
        }

        # Run HDBSCAN
        hdbscan_tfidf_labels, hdbscan_tfidf_dbcv = run_hdbscan(
            tfidf_matrix.toarray(), HDBSCAN_MIN_CLUSTER_SIZE
        )
        results["hdbscan_tfidf"] = {
            "labels": hdbscan_tfidf_labels,
            "dbcv_score": hdbscan_tfidf_dbcv,
            "algorithm": "HDBSCAN",
            "representation": "TF-IDF",
        }
    else:
        print("\nSkipping clustering for TF-IDF (not loaded).")

    # 3. Run Clustering on Embeddings
    print("\n--- Clustering on Embedding Representations ---")
    for model_key, embeddings in embeddings_dict.items():
        print(f"\n=== Processing Embeddings: {model_key} ===")
        # Determine optimal K for this embedding using Silhouette
        print(f"Finding optimal K for KMeans on Embeddings ({model_key})...")
        # Unpack all 3 return values, even if scores_dict isn't used here directly
        optimal_k_emb, _, _emb_scores = find_optimal_k_silhouette(embeddings, ELBOW_MAX_K)
        print(f"Optimal K for Embeddings '{model_key}' (Silhouette): {optimal_k_emb}")

        # Run KMeans with optimal K
        kmeans_emb_labels, kmeans_emb_sil = run_kmeans(embeddings, optimal_k_emb)
        results[f"kmeans_{model_key}"] = {
            "labels": kmeans_emb_labels,
            "silhouette": kmeans_emb_sil,  # Note: run_kmeans recalculates this
            "algorithm": "KMeans",
            "representation": f"Embeddings ({model_key})",
        }

        # Run HDBSCAN
        hdbscan_emb_labels, hdbscan_emb_dbcv = run_hdbscan(embeddings, HDBSCAN_MIN_CLUSTER_SIZE)
        results[f"hdbscan_{model_key}"] = {
            "labels": hdbscan_emb_labels,
            "dbcv_score": hdbscan_emb_dbcv,
            "algorithm": "HDBSCAN",
            "representation": f"Embeddings ({model_key})",
        }

    # 4. Save Results
    save_results(results, OUTPUT_DIR)

    # 5. Visualize Results (Optional)
    if VISUALIZE:
        print("\n--- Generating PCA Visualizations ---")

        # Determine which data matrix to use for each result key
        for result_key, result_data in results.items():
            labels = result_data["labels"]
            algorithm = result_data["algorithm"]
            representation_info = result_data["representation"]
            data_matrix = None
            plot_title = f"{algorithm} on {representation_info}"
            plot_filename = os.path.join(OUTPUT_DIR, f"{result_key}_pca3d.png")

            if representation_info == "TF-IDF":
                if tfidf_matrix is not None:
                    data_matrix = tfidf_matrix
                else:
                    print(f"Skipping PCA plot for {result_key} (TF-IDF data not loaded)")
                    continue
            elif representation_info.startswith("Embeddings"):
                # Extract model key like "spacy_en_sm" from "Embeddings (spacy_en_sm)"
                model_key = representation_info[len("Embeddings (") : -1]
                if model_key in embeddings_dict:
                    data_matrix = embeddings_dict[model_key]
                else:
                    print(
                        f"Skipping PCA plot for {result_key} (Embedding data for '{model_key}' not found)"
                    )
                    continue
            else:
                print(
                    f"Skipping PCA plot for {result_key} (Unknown representation type: '{representation_info}')"
                )
                continue

            # Generate PCA plot for this result
            visualize_clusters_pca_3d(data_matrix, labels, plot_title, plot_filename)

        # UMAP Visualization code removed as requested
        # print("\n--- UMAP Visualizations Skipped ---")

    print("\n--- Clustering Script Finished ---")
